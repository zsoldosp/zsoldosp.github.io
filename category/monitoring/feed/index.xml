<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Do. Reflect. Learn. Repeat!</title>
    <link>http://blog.zsoldosp.eu</link>
    <description>Excercises in public learning</description>
    <pubDate>Sat, 08 Mar 2014 10:51:52 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Adding New Quality Checks to the Build</title>
      <link>http://blog.zsoldosp.eu/2014/03/07/adding-new-quality-checks-to-the-build/</link>
      <pubDate>Fri, 07 Mar 2014 18:17:00 CET</pubDate>
      <category><![CDATA[monitoring]]></category>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">HCF28j7b4oOFWdUd2ZzJi66jW7s=</guid>
      <description>Adding New Quality Checks to the Build</description>
      <content:encoded><![CDATA[
          <p><small><strong>update</strong> (2014-03-08): added reference to podcast time segments
and clarified ambiguity </small></p>
<p>I just got around to listening to <a href="http://theshipshow.com/2014/02/scaling-your-self-service-as-a-service/">The Ship Show #37</a>, and
one of the topics they've discussed (47:00-48:00, 49:30-50:00) was
adding new quality checks to the build. Or to be more precise, they've
<strike>discussed</strike> talked about a case which exhibits the
anti-pattern way of adding new checks to the build, namely:</p>
<ol>
<li>Find a new quality check (e.g.: let's not have compiler warnings)</li>
<li>Get team buy in </li>
<li>Modify the build to fail on these violations</li>
<li>Have a broken build for the coming months - and have developers
   ignore <strong>real</strong> build failures (<em>ah, it's broken only due to the new
   checks!</em>)</li>
<li>Abandon this new quality check so that a failed build actually
   means the build is actually broken</li>
</ol>
<p>One way to avoid the above scenario is to focus on gradual improvement,
i.e.: after steps #1 and #2 (which shouldn't be skipped!), in step #3, 
instead of a <em>zero-tolerance</em> policy, you <strong>set the limit for the new check 
violations at the current level</strong> - i.e.: the team commits to not making the
situation any worse, and trying to improve it. Exceeding this limit will 
make the build fail, and thus there are no more false alarms - all build 
failures have to be taken seriously.</p>
<p>If your monitoring/build tool doesn't support "continuously decreasing limits"
out of the box, you'll have to adjust this limit manually, let's say every morning.
Be careful to only ever <em>lower</em> the limit though!</p>
<p>And there are other <a href="/2014/01/19/green-build-surprises/">things you should be monitoring about your build</a>
too!</p>
          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2014/03/07/adding-new-quality-checks-to-the-build/">Adding New Quality Checks to the Build</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
    <item>
      <title>Green Build Surprises</title>
      <link>http://blog.zsoldosp.eu/2014/01/19/green-build-surprises/</link>
      <pubDate>Sun, 19 Jan 2014 11:30:00 CET</pubDate>
      <category><![CDATA[monitoring]]></category>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">Fp298W8YFUYXUTLCJhSRMzFmERU=</guid>
      <description>Green Build Surprises</description>
      <content:encoded><![CDATA[
          <p>Recently while working on our build process, I ran into an unexpected
surprise - the build server reported no error (i.e.: it was displaying a
green status, since all tests were passing), yet there was a major issue
- not all of our tests were run!</p>
<p>Given that the situation was caused by working on the build itself, the
issue was discovered in time (<strike>though somewhat accidentally</strike>
during <a href="https://en.wikipedia.org/wiki/Exploratory_testing">exploratory testing</a>). However, this was a
nice reminder of the problem with the "unknown unknowns" - now that we
know of this issue, of course we have configured alerting for the case
when the current build ran less tests than the build before. But I
wonder how many times we may have released on a green build with
regression bugs...</p>
<p>Asking some others and looking back on my past experiences, this topic
hasn't come up, so while I hope it is obvious and not news for you, dear
reader, I blog it just in case I wasn't the last to get the memo...</p>
<p>And it turns out that every now and then we do mess up and commit code
that makes some tests invisible to the test runner... So it wasn't just
a one-off thing!</p>
          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2014/01/19/green-build-surprises/">Green Build Surprises</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
  </channel>
</rss>
