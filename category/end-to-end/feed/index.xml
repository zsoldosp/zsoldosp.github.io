<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Do. Reflect. Learn. Repeat!</title>
    <link>http://blog.zsoldosp.eu</link>
    <description>Excercises in public learning</description>
    <pubDate>Mon, 28 Oct 2013 07:09:40 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Going Beyond Regression - What Other Benefits could End-to-End Testing Provide?</title>
      <link>http://blog.zsoldosp.eu/2013/10/28/going-beyond-regression-what-other-benefits-could-end-to-end-testing-provide/</link>
      <pubDate>Mon, 28 Oct 2013 08:05:00 CET</pubDate>
      <category><![CDATA[untested ideas]]></category>
      <category><![CDATA[code]]></category>
      <category><![CDATA[business analysis]]></category>
      <category><![CDATA[testing]]></category>
      <category><![CDATA[end-to-end]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">1ojU7m2klQVoaVtPAyBIp7nHdds=</guid>
      <description>Going Beyond Regression - What Other Benefits could End-to-End Testing Provide?</description>
      <content:encoded><![CDATA[<p>In last week's post, I gave a few examples about when <a href="/2013/10/23/a-new-look-at-end-to-end-testing-polymorphic-and-fast/">Polymorphic
End-to-End Testing</a> testing makes sense. In this post, I would
like to take a step back and list a few additional benefits that could
be derived from end to end testing in general, regardless of the fashion
they were written in.</p>
<p><em>Note: some of these points can also apply to non-end-to-end tests too,
that were written in a black box style with proper test abstractions. 
While it could be argued that then the system is that one component, I
would rather just focus on the benefits we can derive on top of the 
regression / specification of the system.</em></p>
<p><em>Disclaimer:</em> note this post has been filed under <a href="/category/untested-ideas/">untested ideas</a>
 - they sound good, but I haven't gotten around to implementing all of
them.</p>
<h2 id="big-refactorings-and-rewrites">Big Refactorings and Rewrites</h2>
<p>Change is inevitable, and they often violate previous assumptions. 
Sometimes whole components (or systems) have to be rewritten. Having a 
set of tests that operate on a much higher abstraction level (e.g.: HTTP
GET/POST requests) can provide the required safety net to avoid 
regressions and making sure all relevant scenarios are addressed.</p>
<p>Some changes where this Page (Application) Object abstraction has helped
us:</p>
<ul>
<li>when converting a single page checkout process to a multi-step 
    wizard style checkout</li>
<li>when the article numbers used in our system changed</li>
<li>when we had to synchronize data into a new system - we could just 
    expand our assertions in the end-to-end tests to make sure that 
    every known scenario is written correctly into the new system</li>
</ul>
<p><a href="http://blog.8thlight.com/uncle-bob/2013/09/23/Test-first.html">It's easy to reconstruct a system from its tests, but much harder to do
it the other way around</a>. It has been
great to only adjust a few driver API methods and get the same amount of
functional coverage as before, without having to rewrite the test suite.</p>
<h2 id="forces-the-team-to-think-about-the-user-interface-and-experience">Forces the team to think about the user interface (and experience)</h2>
<p>While it is not required, it often made us reduce the complexity in 
the UI - when we find that a certain step is being exercised by a 
method with a single parameter from the tests, yet that method then 
derives a bunch of additional parameters to POST against the page, it
suggests one of two things: </p>
<ol>
<li>we are missing some test cases for these extra parameters</li>
<li>maybe we don't need these parameters to be provided by the end user,
    but we could derive them in the application too.</li>
</ol>
<p>Often the helper methods created can expose the need for additional 
support interfaces that won't probably come up during the specifications
phase, only after go-live.<br />
</p>
<p>Last, but not least, end-to-end gives us tests for the UI, yet the tests
remain maintainable - usually a single test API method is all that needs
to be fixed after template changes (and designers are even harder to get
to write tests than developers :)). <a href="http://ayende.com/blog/160929/on-failing-tests">Don't be afraid of many test 
failures!</a></p>
<h2 id="correlate-tests-with-other-business-metrics">Correlate tests with other business metrics</h2>
<p>While I recall people suggesting we run applications with a <a href="https://en.wikipedia.org/wiki/Code_coverage">coverage 
profiler attached</a>, the performance penalty is usually 
prohibiting.</p>
<p>However, I haven't yet seen a web application without a ton of external
metrics related to the urls in the app. If our tests are written against
urls too, after some data munging (primary keys and actual form values
surely won't match test values exactly, but translating them to <code>GET to 
view 1</code>, <code>POST to view 2</code>) we can correlate our tests with these 
metrics. </p>
<p>Some such metrics:</p>
<ul>
<li>application (webserver) access logs</li>
<li>Google Analytics or equivalent</li>
<li>...</li>
</ul>
<p>What can we learn from these correlations/comparisons?</p>
<ul>
<li>are we concentrating our tests in the least visited areas?</li>
<li>are we testing what our users are doing? Sure, it's nice that in our
    tests people sequentially finish their checkout, without wondering
    off the known path, but is this how they behave in production?</li>
</ul>
<h2 id="testcase-similarity-analysis">TestCase similarity analysis</h2>
<p>Some test scenarios will come up in multiple aspects of the system. 
Placing an order will trigger a bunch of actions in other modules - 
fulfillment, customer profile updates, marketing classification, invoice
rendering, notification emails, etc.</p>
<p>Sometimes these features are added with big time gaps in between, maybe
even the team members have changed over time. The ability to compare 
the requests the different TestCases make, and say that these two 
(three, four, etc.) TestCases seem to execute the same kind of requests
up to a point as the TestCase being added, but they also have the 
following extra paths they all execute, but the new TestCase doesn't...
Causing the developer to realize - of course, there are special rules
for orders from educational institutions!</p>
<h2 id="reducing-the-gap-between-end-user-error-reports-and-tests">Reducing the gap between end user error reports and tests</h2>
<p>Probably this is the least unexpected idea in the list, but worth 
stating nonetheless.</p>
<h2 id="in-place-help-for-trusted-users">In place help (for trusted users)</h2>
<p>Sure, this might require careful considerations, but giving the users 
the ability to browse the test cases/methods that matches their workflow
up to the current page, in a searchable fashion (if I place an order 
like this now, when will the X email be sent) could greatly reduce 
support work for the developer team. <em>Note: the purpose of this is not 
to isolate the developers from the users!!!</em></p>
<h2 id="always-up-to-date-screenshots-and-videos">Always up-to-date screenshots and videos</h2>
<p>Those manuals that have screenshots from many releases ago... Adding (or
marking) some test cases to be linked against documentation sections, and
having the tests actually take screenshots about them or record them as 
video sounds like a pretty useful idea for me. One could go further, 
such as highlighting the values/input fields where the test sends 
input, and the parts of the page that are asserted against...</p>
<p>And if the support team has access to the same API, they could even 
create these screenshots/videos for the customer as they are answering
their question (Here, take a look at this video, this is how the thing
you asked for is done).</p>
<hr />
<p>There must be many more ideas out there - let me know about them, and 
I'm happy to add them to this list (or link to wherever you published
it)!</p>]]></content:encoded>
    </item>
    <item>
      <title>A New Look At End-to-End Testing - Polymorphic and Fast</title>
      <link>http://blog.zsoldosp.eu/2013/10/23/a-new-look-at-end-to-end-testing-polymorphic-and-fast/</link>
      <pubDate>Wed, 23 Oct 2013 19:28:00 CEST</pubDate>
      <category><![CDATA[untested ideas]]></category>
      <category><![CDATA[end-to-end]]></category>
      <category><![CDATA[code]]></category>
      <category><![CDATA[testing]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">ity9bCalQkCuoSztWSPkSrQ8XGs=</guid>
      <description>A New Look At End-to-End Testing - Polymorphic and Fast</description>
      <content:encoded><![CDATA[<p>At the end of this post, there is a list of reasons why to work with 
end-to-end tests, but first please consider the post's idea on its own.
After that, I'm glad to have discussions about alternative/better 
solutions to the described (or omitted) contexts.</p>
<p>Please, read it first ;)</p>
<h1 id="terms-used">Terms used</h1>
<p>The below points are not intended to be full blown definitions, but
rather pointers.</p>
<ul>
<li>End-to-End testing - as <a href="http://gojko.net/2010/03/31/tdd-with-complex-infrastructures/">Nat Pryce has said in one of his 
    presentations</a>, the ends are always farther apart than one 
    thinks they are. The purpose is to execute the tests through as much
    of the application stack as possible - from the front end at least
    till the storage mechanism.</li>
<li>Polymorphism - I use it here mostly in the way demonstrated by the 
    <a href="https://en.wikipedia.org/wiki/Liskov_Substitution_Principle">Liskov Substitution Principle</a> - code written against an abstraction
    should be unaffected regardless of the concrete implementation of 
    that abstraction is given to it.</li>
</ul>
<h1 id="polymorphic-tests-we-are-already-doing-it">Polymorphic tests - we are already doing it</h1>
<p>Some of us already run the same tests on top of different code - we can
have multiple build platforms (x86 and x64, Windows and Linux, Python 2 
and 3, etc.), multiple configurations (SQLite and PostgreSQL), as well
as multiple versions of our dependent libraries (stable, latest release, 
and latest). What's common though among these scenarios is that the 
polymorphism happens largely <em>outside</em> of our codebase, and we don't 
have to think much about it when writing tests. </p>
<p>An example of executing multiple drivers <em>inside</em> our code is the use of 
Selenium tests - the same tests are run against Chrome, Firefox, etc. 
While each of the drivers is testing on the same level (Web UI), the 
actual browser drivers have different implementations, exposed via a 
common abstraction level - DOM selectors and event invocations.</p>
<p>Of course, most test code uses some level of abstraction to separate
the test logic from the actual page implementations.</p>
<h1 id="abstractions-page-objects">Abstractions - Page objects</h1>
<p>The <a href="http://martinfowler.com/bliki/PageObject.html">Page Object</a> pattern is used to help creating maintainable tests.
Instead of writing tests coupled to the implementations (go to this 
concrete url, wait <code>N</code> seconds for it to load, find and select the form
elements for username and password, etc.), these implementation details 
are hidden behind well named methods (e.g.: <code>open_login_form</code>, 
<code>login_with_credentials</code>, etc.), and thus are domain (client) friendly
and readable. And Page Objects can be composed together to build 
Application Objects.</p>
<p>Similar abstraction is used by the various Acceptance Testing tools,
such as FitNesse, Cucumber, and the other Gherkin tools - the spec texts
contain terms and values important for the business domain, and there
is separate code translating the spec's values and terms to call into 
the application and transforming its state into a format
expected by the tool.</p>
<h1 id="stripped-down-tests-only-the-script">Stripped down tests - only the script</h1>
<p>As seen above, the AT tools separate application logic from the test 
scenario's description.</p>
<p>Assertions have also been separated from test cases - either by developer 
choice, choosing to use a separate Assertions library like Hamcrest, 
instead of the unit testing library's own <code>assertFoo</code> methods), or
explicitly (<a href="http://visionmedia.github.io/mocha/#assertions">Mocha ships without an assertions library</a>).</p>
<p>Thus tests can really be focused just on the scenario being tested.</p>
<h1 id="fast-tests">Fast tests</h1>
<p>The single biggest disadvantage of <a href="http://www.confreaks.com/videos/641-gogaruco2011-fast-rails-tests">end to end tests is their speed</a>. They are slow. And the more of them there are, the slower they are.</p>
<p>This is one reason why the <a href="http://martinfowler.com/bliki/TestPyramid.html">Test Pyramid</a> recommends not having 
too many of them. Many architectural approaches (<a href="http://alistair.cockburn.us/Hexagonal+architecture">hexagonal</a>, DDD, etc.) 
suggest keeping a lightweight core application, and to attach the 
persistence and UI layers to it at its boundaries, leaving these ports 
and adapters lightweight too. Most of the testing then happens against
the core, dependency independent code, making the tests fast. </p>
<h1 id="fast-end-to-end-tests">Fast end to end tests</h1>
<p>Drumroll... we'll do a bit of cheating, of course. </p>
<p>Not all the tests have to run every single time. Performance tests are
usually not done when TDDing - that kicks in either later in 
the deployment pipeline, or runs daily. Teams organize their tests into
fast, smoke, and slow suites. Locally (and as the first step in the build
process) only the fast and smoke tests are run.</p>
<p>Putting all the above together means that writing systems with two 
self- contained cores (the app domain itself and the test scenarios) 
easily lends itself to end-to-end testing that can be run on multiple 
configurations, to give confidence that the app works with all its 
components and dependencies in production - yet enable fast feedback
required for developers. The same tests can be run:</p>
<ul>
<li>directly against the core application with mocks, stubs, etc.</li>
<li>through the app's (http) UI via the given frameworks/libraries testing 
  tools (e.g.: <code>django.test.client.Client</code>) with an in-memory database</li>
<li>through selenium against the full stack</li>
</ul>
<p>And of course, we can mix and match - selenium against SQLite, etc.</p>
<p>While TDDing, one can run the tests only against the fast core, after
that is complete, we can run the relevant tests with the end-to-end
driver, fix any mistakes that occur, check in, and let the build server
run all the integrated tests (using existing build practices to 
achieve speed)!</p>
<h2 id="in-which-contexts-could-it-make-sense">In Which Contexts Could It Make Sense?</h2>
<p>Thank you for reading this far - assuming you didn't just scroll 
ahead :)</p>
<p>The below list is by no means exhaustive, and as mentioned in the
introduction, there might be alternative approaches (please, let me
know!) - it's not a coincidence this blog is called "Exercises in 
public learning"! </p>
<p>With that out of the way, here are some contexts where this approach
could make sense:</p>
<ul>
<li>
<p>Working with a team where the skills both for testing and for writing
    good code are (yet) missing (<a href="http://5whys.com/blog/the-3-maturity-stages-of-a-software-team-and-how-scrum-fails.html">Chaotic team phase</a>).</p>
<p>As the joke goes, the only way to eat the elephant is one bite at a 
time. Same goes for learning - people can be overwhelmed to make 
the mental jump from manual to automated testing - throwing in good
programming practices can be too much.</p>
<p>Getting started with end to end tests that have decoupled driver
methods (even if on the <code>TestCase</code> class itself) is a great start - by
the time the tests become slow, if the team is bought into the idea
of automated testing, it can be refactored towards a core domain -
and inside that domain there still doesn't have to be proper clean
code (one step at a time).</p>
<p>In brief: for slow, gradual improvement.</p>
</li>
<li>
<p>The app actually has multiple interfaces for the same thing.</p>
<p>It can be due to A/B testing, or simply to accommodate the different
needs of different users (e.g.: for a webshop - there is the public
shop, the internal UI geared at the company's sales people, and the
API), multi-platform application (e.g.: mobile and desktop web, iOS and 
Android), etc.</p>
<p>If you test the checkout process end-to-end, then running the same
set of tests against each UI makes sense too - a single set of tests
to maintain and you know immediately whether all features work 
across all the views.</p>
</li>
<li>
<p>Catching unexpected bugs.</p>
<p>There is a class of bugs that can be caught by rigor, but I do slip
occasionally, ending up in a place where the unit tests are all 
green, but the application itself doesn't actually work.</p>
<p>Some real life such bugs I have run into:</p>
<ul>
<li>forgetting to place the actual input element on the page</li>
<li>encoding-persistence issues - an utf-8 database with a column
    that is windows-1250 encoded is ... unexpected</li>
<li>synchronizing data with another database where after the 
    required mappings it turned out said other database truncates
    our data</li>
</ul>
<p>All of the above can be addressed retroactively via adding targeted
tests for that specific integration point, but if we are already
testing the corner cases (length, encoding, etc.) in our code, it is 
nicer not to have to learn about these "unknown unknowns" from 
production problems...</p>
</li>
<li>
<p>Finally some related posts from other people:</p>
<ul>
<li>Ayende has multiple posts: on <a href="http://ayende.com/blog/154273/limit-your-abstractions-and-how-do-you-handle-testing">swapping out the infrastructure</a>, <a href="http://ayende.com/blog/4218/scenario-driven-tests">separating assertions from tests</a>, and <a href="http://ayende.com/blog/4217/even-tests-has-got-to-justify-themselves">about which tests add value in his opinion</a></li>
<li><a href="http://codebetter.com/sebastienlambla/2013/07/11/unit-testing-is-out-vertical-slice-testing-is-in/">Sebastien Lambla on Vertical Testing</a></li>
</ul>
</li>
</ul>
<p>There is much more to be said about other <a href="/2013/10/28/going-beyond-regression-what-other-benefits-could-end-to-end-testing-provide/">benefits of end to end 
testing</a>, but this post is already too long, so that will have to wait
for <a href="/2013/10/28/going-beyond-regression-what-other-benefits-could-end-to-end-testing-provide/">another post</a> (while waiting, you can read <a href="http://codemanship.co.uk/parlezuml/blog/?postid=1183">Jason Gorman's 101 Uses for Polymorphic testing</a>)!</p>
<p>P.S.: I would like to thank (in (first name based) alphabetical order): 
<a href="https://twitter.com/ajmolenaar">Arjan Molenaar</a>,
<a href="https://twitter.com/sietstweets">Cirilo Wortel</a>,
<a href="http://douglassquirrel.com/">Douglas Squirrel</a>,
<a href="http://twitter.com/jtf">Jeffrey Frederick</a>,
<a href="https://twitter.com/KishenPanday">Kishen Simbhoedatpanday</a>,
<a href="http://twitter.com/marcoemrich">Marco Emrich</a>,
<a href="https://twitter.com/mfeathers">Michael Feathers</a>,
and of course my colleagues at <a href="http://www.paessler.com">Paessler AG</a> -  I much appreciate that you 
all listened to me while I tried to figure out how to explain this and 
gave feedback both about the content and the format (*). Thank you!</p>
<hr />
<p>(*) just to be crystal clear, this does not mean they endorsed it, 
just that they listened and gave feedback!</p>]]></content:encoded>
    </item>
  </channel>
</rss>
