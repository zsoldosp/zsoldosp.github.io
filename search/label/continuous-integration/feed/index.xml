<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Do. Reflect. Learn. Repeat!</title>
    <link>http://blog.zsoldosp.eu</link>
    <description>Excercises in public learning</description>
    <pubDate>Mon, 18 Apr 2016 12:32:00 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Adding New Quality Checks to the Build</title>
      <link>http://blog.zsoldosp.eu/2014/03/07/adding-new-quality-checks-to-the-build/</link>
      <pubDate>Fri, 07 Mar 2014 18:17:00 CET</pubDate>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[monitoring]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">HCF28j7b4oOFWdUd2ZzJi66jW7s=</guid>
      <description>Adding New Quality Checks to the Build</description>
      <content:encoded><![CDATA[
          <p><small><strong>update</strong> (2014-03-08): added reference to podcast time segments
and clarified ambiguity </small></p>
<p>I just got around to listening to <a href="http://theshipshow.com/2014/02/scaling-your-self-service-as-a-service/">The Ship Show #37</a>, and
one of the topics they've discussed (47:00-48:00, 49:30-50:00) was
adding new quality checks to the build. Or to be more precise, they've
<strike>discussed</strike> talked about a case which exhibits the
anti-pattern way of adding new checks to the build, namely:</p>
<ol>
<li>Find a new quality check (e.g.: let's not have compiler warnings)</li>
<li>Get team buy in </li>
<li>Modify the build to fail on these violations</li>
<li>Have a broken build for the coming months - and have developers
   ignore <strong>real</strong> build failures (<em>ah, it's broken only due to the new
   checks!</em>)</li>
<li>Abandon this new quality check so that a failed build actually
   means the build is actually broken</li>
</ol>
<p>One way to avoid the above scenario is to focus on gradual improvement,
i.e.: after steps #1 and #2 (which shouldn't be skipped!), in step #3, 
instead of a <em>zero-tolerance</em> policy, you <strong>set the limit for the new check 
violations at the current level</strong> - i.e.: the team commits to not making the
situation any worse, and trying to improve it. Exceeding this limit will 
make the build fail, and thus there are no more false alarms - all build 
failures have to be taken seriously.</p>
<p>If your monitoring/build tool doesn't support "continuously decreasing limits"
out of the box, you'll have to adjust this limit manually, let's say every morning.
Be careful to only ever <em>lower</em> the limit though!</p>
<p>And there are other <a href="/2014/01/19/green-build-surprises/">things you should be monitoring about your build</a>
too!</p>
          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2014/03/07/adding-new-quality-checks-to-the-build/">Adding New Quality Checks to the Build</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
    <item>
      <title>Green Build Surprises</title>
      <link>http://blog.zsoldosp.eu/2014/01/19/green-build-surprises/</link>
      <pubDate>Sun, 19 Jan 2014 11:30:00 CET</pubDate>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[monitoring]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">Fp298W8YFUYXUTLCJhSRMzFmERU=</guid>
      <description>Green Build Surprises</description>
      <content:encoded><![CDATA[
          <p>Recently while working on our build process, I ran into an unexpected
surprise - the build server reported no error (i.e.: it was displaying a
green status, since all tests were passing), yet there was a major issue
- not all of our tests were run!</p>
<p>Given that the situation was caused by working on the build itself, the
issue was discovered in time (<strike>though somewhat accidentally</strike>
during <a href="https://en.wikipedia.org/wiki/Exploratory_testing">exploratory testing</a>). However, this was a
nice reminder of the problem with the "unknown unknowns" - now that we
know of this issue, of course we have configured alerting for the case
when the current build ran less tests than the build before. But I
wonder how many times we may have released on a green build with
regression bugs...</p>
<p>Asking some others and looking back on my past experiences, this topic
hasn't come up, so while I hope it is obvious and not news for you, dear
reader, I blog it just in case I wasn't the last to get the memo...</p>
<p>And it turns out that every now and then we do mess up and commit code
that makes some tests invisible to the test runner... So it wasn't just
a one-off thing!</p>
          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2014/01/19/green-build-surprises/">Green Build Surprises</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
    <item>
      <title>Inversion of Control for Continuous Integration</title>
      <link>http://blog.zsoldosp.eu/2012/01/inversion-of-control-for-continuous.html</link>
      <pubDate>Fri, 17 Feb 2012 20:36:00 CET</pubDate>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[ioc]]></category>
      <category><![CDATA[software]]></category>
      <category><![CDATA[version control]]></category>
      <guid isPermaLink="false">http://blog.zsoldosp.eu/2012/01/inversion-of-control-for-continuous.html</guid>
      <description>Inversion of Control for Continuous Integration</description>
      <content:encoded><![CDATA[
          
<h1>Problem Description</h1><br/><br/><p>Our build structure is pretty stable, but the exact content of the steps varies as we discover more smoke tests that we'd like to add to, or when we rearrange the location of these checks.</p><br/><br/><p>The CI servers I've used made this a rather cumbersome process:</p><br/><br/><ul class="simple"><br/><br/><li>First, I have to leave my development environment to go to the build servers configuration of choice - most of the time it is a web interface, and for some it is a config file</li><br/><br/><li>I have to point and click, and if it's a shell script, I have to make my modifications without syntax highlighting (for the config files usually take the shell command to execute as a string, so no syntax highlighting)</li><br/><br/><li>If it's a web interface, I have (or had) no versioning/backup/diff support for my changes (config files are better in this aspect).</li><br/><br/><li>If it's a config file, then I need to get it to the build server (we version control our config files), so that's at least one more command</li><br/><br/><li>I need to save my changes, and run a whole build to see whether my changes worked, which is a rather costly thing.</li><br/><br/><li>Most places have only one build server, so when I'm changing the step, I either edit the real job (bad idea) or make a copy of it, edit it, and then integrate it back to the real job. Of course, integrating back means: copy and paste.</li><br/><br/><li>If the build failed, I need to go back to the point and click and no syntax highlighting step to fix the failures</li><br/><br/><li>Last, but not least, with web interfaces, concurrent modifications of a build step lead to nasty surprises!</li><br/><br/></ul><br/><br/><br/><br/><h1>Normal development workflow</h1><br/><br/><ul class="simple"><br/><br/><li>I have an idea what I want to do</li><br/><br/><li>I write the tests and code to make it happen</li><br/><br/><li>I run the relevant tests and repeat until it's working</li><br/><br/><li>I check for source control updates</li><br/><br/><li>I run the pre-commit test suite (for dvcs people: pre-push)</li><br/><br/><li>Once all tests pass I commit, and move on to the next problem</li><br/><br/></ul><br/><br/><p>Quite a contrast, isn't it? And even the concurrent editing problem is solved!</p><br/><br/><br/><h1>Quick'n'Dirty Inversion of Control for builds</h1><br/><br/><p>Disclaimer: the solution described below is a really basic, low tech, proof of concept implementation.</p><br/><br/><p>Since most build servers at the end of the day</p><br/><br/><ul class="simple"><br/><br/><li>invoke a shell command</li><br/><br/><li>and interpret exit codes, <tt class="docutils literal">stdout</tt>, <tt class="docutils literal">stderr</tt>, and/or log files</li><br/><br/></ul><br/><br/><p>we defined the basic steps (update from version control, initialize database, run tests, run checks, generate documentation, notify) using the standard build server configuration, but the non-built in steps (all, except the version control update and the notification) are defined to invoke a shell script that resides in the project's own repository (e.g.: under bin/ci/oncommit/runchecks.sh). These shell scripts' results can be interpreted by the standard ways CI servers are familiar with - exceptions and stack traces, (unit)test output, and exit codes.</p><br/><br/><br/><h1>Benefits</h1><br/><br/><ul class="simple"><br/><br/><li>adding an extra smoke test doesn't require me to break my flow, and I can more easily test my changes locally and integrating it back into the main build means just committing it to the repository, and the next build will already pick this up</li><br/><br/><li>I can run the same checks locally if I would like to</li><br/><br/><li>if I were to support a bigger team/organization with their builds, this would make it rather easy to maintain a standard build across teams, yet allow each of them to customize their builds as they see it fit</li><br/><br/><li>if I were to evaluate a new build server product, I could easily and automatically see how it would work under production load, just by:<ul><br/><br/><li>creating a single parameterized build (checkout directory, source code repository)</li><br/><br/><li>defining the schedule for each build I have</li><br/><br/><li>and then replaying the past few weeks/months load - OK, I still would need to write the script that would queue the builds for the replay, but it still is more effective than to run the product only with a small pilot group and then see it crash under production load</li><br/><br/></ul><br/><br/></li><br/><br/></ul><br/><br/><br/><h1>Shortcomings, Possible Improvements</h1><br/><br/><p>As said, the above is a basic implementation, but has served a successful proof of concept for us. However, our builds are simple:</p><br/><br/><ul class="simple"><br/><br/><li>no dependencies between the build steps, it is simply chronological</li><br/><br/><li>no inter-project dependencies, such as component build hierarchy (if the server component is built successfully, rerun the UI component's integration tests in case the server's API changed, etc.)</li><br/><br/><li>the tests are executed in a single thread and process, on a single machine - no parallelization or sharding</li><br/><br/></ul><br/><br/><p>All of the above shortcomings could be addressed by writing a build server specific interpreter that would read our declarative build config file (map steps to scripts, define step/build dependencies/workflows), and would redefine the build's definition on the server. By creating a standard build definition format, we could just as easily move our builds between different servers as we can currently do with blogs - pity Google is not a player in the CI space, so the <a class="reference external" href="http://www.dataliberation.org/">Data Liberation Front</a> cannot help :).</p><br/><br/><br/><h1>Questions</h1><br/><br/><p>Does this idea make sense for you? Does such a solution already exist? Or are the required building blocks available? Let me know in the comments!</p>
<hr><ol>
<li id="1">
    <strong><a href="http://www.blogger.com/profile/06859781419645954688">bridge</strong></a> on <em>2012/02/21 14:55:14</em>: What we do at our build server is something similar to this:<br/><br/>We have ant steps defined, e.g. build, run checkstyle, run unit tests. We have different Hudson jobs for the same branch, one for each purpose.<br/><br/>Applying this environment for your scenario (tell me if I missed something from the problem) would look like this:<br/>- we could create a new job for the integration tests.<br/>- for changing the integration tests, we could write new tests and add them to the corresponding ant step. We could write the code and the ant xml in our favorite IDE. We could run them in our environment before committing it.<br/>- we can use the output of the build step in the same way we usually do. This is right now stop/go; but I think there are other ways to do this out there.
<ol>
<li id="1-1">
    <strong><a href="/">Peter Zsoldos</strong></a> on <em>2012/02/21 19:41:11</em>: Yup, your example achieves the same purpose as what I described. I have used the shell script in this example because a) I haven't worked on java projects in ages and I forgot it exists b) we have a number of scripts (both shell and django runscripts/management commands as well) that we reuse <br/><br/>When you say multiple jobs, you mean for one project (branch) you have 4-5 jobs or one job with multiple parallel steps?
</li></ol></li><li id="2">
    <strong><a href="http://www.blogger.com/profile/06859781419645954688">bridge</strong></a> on <em>2012/02/22 13:55:01</em>: When I say multiple jobs, we have 2 jobs for each branch. I don't know if we can make Hudson smarter - this is how the experts set it up initially, and nobody ever questioned it.<br/><br/>Having predefined scripts, as placeholders to invoke other scripts - that is poor-mans-ant: you have a layer of abstraction between the CI server and the scripts that are actually running. Say for instance, Hudson invokes the smoketest.sh, which invokes the low-level step, something like this:<br/><br/>#!/bin/bash<br/>compile.sh<br/>dist.sh<br/>run-unit-tests.sh<br/>check-login-ui.sh<br/>check-the-crazy-rollback-use-case.sh<br/><br/>So all you had to do would be add a new line to this shell/perl file.<br/><br/>Or, in perl (or python, whatever) you can add any other logic<br/>#!/usr/bin/perl<br/>if (system("dist.sh") != 0) {<br/>  exit;<br/>}<br/>if (system("run-unit-tests.sh") != 0) {<br/>  exit;<br/>}<br/>#...<br/><br/>Here you can add alternative and parallel flows too. A bit ugly, programmatic, but works. Even more, you can keep it clean if you keep the actual step's scripts and the flow's scripts in different files.<br/><br/>SoapUI is not a CI solution, but still, it's something similar as it's a testing tool. They suggest to use scripts in order to define custom logic between the test steps. http://www.soapui.org/Functional-Testing/controlling-flow.html
</li></ol>

          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2012/01/inversion-of-control-for-continuous.html">Inversion of Control for Continuous Integration</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
    <item>
      <title>Slides for the Continuous Delivery talk</title>
      <link>http://blog.zsoldosp.eu/2010/10/slides-for-continuous-delivery-talk.html</link>
      <pubDate>Sun, 10 Oct 2010 18:19:00 CEST</pubDate>
      <category><![CDATA[automation]]></category>
      <category><![CDATA[continuous delivery]]></category>
      <category><![CDATA[continuous integration]]></category>
      <category><![CDATA[presentation]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">http://blog.zsoldosp.eu/2010/10/slides-for-continuous-delivery-talk.html</guid>
      <description>Slides for the Continuous Delivery talk</description>
      <content:encoded><![CDATA[
          
I gave a talk on Friday at the firm tech user group about the build/release evolution towards Continuous Delivery. The <a href="http://github.com/zsoldosp/zsoldosp-presentations/raw/28f95532bdb57b7f9a95a358b7b8835d513856ba/ContinuousIntegrationDeployment/continuous-integration-deployment.pptx">slides</a> are up under <a href="http://github.com/zsoldosp/zsoldosp-presentations/">my github presentations project</a>. It was built on top of <a href="http://www.agilealliance.hu/szakmai-anyagok/eloadasok">the Continuous Integration presentation</a> I gave at the<a href="http://www.meetup.com/AgileHungary/"> Agile Hungary Meetup</a> earlier this year (also on github, Hungarian slides only). Contrary to that one, here I was talking of things I haven't yet accomplished and only am looking forward to do, which worried me a bit - however, declaring this upfront was well received and it didn't pressure me from that point on.<div><br/></div><div><div>Though I wasn't as good as <a href="http://www.build-doctor.com/.../continuous-deployment-video-tim-fitz/">Tim Fitz</a> (especially lacking the experience) the talk has generated questions and discussion, which was the purpose, so I'm quite happy with how it went.</div><div><br/></div><div>My only regret is forgetting to record the presentation to review my presentation skills.</div></div>
          <hr />
          The post <a hef="http://blog.zsoldosp.eu/2010/10/slides-for-continuous-delivery-talk.html">Slides for the Continuous Delivery talk</a> first appeared on <a href="http://blog.zsoldosp.eu">http://blog.zsoldosp.eu</a>.
        ]]></content:encoded>
    </item>
  </channel>
</rss>
