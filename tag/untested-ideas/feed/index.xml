<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Do. Reflect. Learn. Repeat!</title>
    <link>http://blog.zsoldosp.eu</link>
    <description>Excercises in public learning</description>
    <pubDate>Wed, 23 Oct 2013 17:59:34 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>A New Look At End-to-End Testing - Polymorphic and Fast</title>
      <link>http://blog.zsoldosp.eu/2013/10/23/a-new-look-at-end-to-end-testing-polymorphic-and-fast/</link>
      <pubDate>Wed, 23 Oct 2013 19:28:00 CEST</pubDate>
      <category><![CDATA[untested ideas]]></category>
      <category><![CDATA[end-to-end]]></category>
      <category><![CDATA[code]]></category>
      <category><![CDATA[testing]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">ity9bCalQkCuoSztWSPkSrQ8XGs=</guid>
      <description>A New Look At End-to-End Testing - Polymorphic and Fast</description>
      <content:encoded><![CDATA[<p>At the end of this post, there is a list of reasons why to work with 
end-to-end tests, but first please consider the post's idea on its own.
After that, I'm glad to have discussions about alternative/better 
solutions to the described (or omitted) contexts.</p>
<p>Please, read it first ;)</p>
<h1 id="terms-used">Terms used</h1>
<p>The below points are not intended to be full blown definitions, but
rather pointers.</p>
<ul>
<li>End-to-End testing - as <a href="http://gojko.net/2010/03/31/tdd-with-complex-infrastructures/">Nat Pryce has said in one of his 
    presentations</a>, the ends are always farther apart than one 
    thinks they are. The purpose is to execute the tests through as much
    of the application stack as possible - from the front end at least
    till the storage mechanism.</li>
<li>Polymorphism - I use it here mostly in the way demonstrated by the 
    <a href="https://en.wikipedia.org/wiki/Liskov_Substitution_Principle">Liskov Substitution Principle</a> - code written against an abstraction
    should be unaffected regardless of the concrete implementation of 
    that abstraction is given to it.</li>
</ul>
<h1 id="polymorphic-tests-we-are-already-doing-it">Polymorphic tests - we are already doing it</h1>
<p>Some of us already run the same tests on top of different code - we can
have multiple build platforms (x86 and x64, Windows and Linux, Python 2 
and 3, etc.), multiple configurations (SQLite and PostgreSQL), as well
as multiple versions of our dependent libraries (stable, latest release, 
and latest). What's common though among these scenarios is that the 
polymorphism happens largely <em>outside</em> of our codebase, and we don't 
have to think much about it when writing tests. </p>
<p>An example of executing multiple drivers <em>inside</em> our code is the use of 
Selenium tests - the same tests are run against Chrome, Firefox, etc. 
While each of the drivers is testing on the same level (Web UI), the 
actual browser drivers have different implementations, exposed via a 
common abstraction level - DOM selectors and event invocations.</p>
<p>Of course, most test code uses some level of abstraction to separate
the test logic from the actual page implementations.</p>
<h1 id="abstractions-page-objects">Abstractions - Page objects</h1>
<p>The <a href="http://martinfowler.com/bliki/PageObject.html">Page Object</a> pattern is used to help creating maintainable tests.
Instead of writing tests coupled to the implementations (go to this 
concrete url, wait <code>N</code> seconds for it to load, find and select the form
elements for username and password, etc.), these implementation details 
are hidden behind well named methods (e.g.: <code>open_login_form</code>, 
<code>login_with_credentials</code>, etc.), and thus are domain (client) friendly
and readable. And Page Objects can be composed together to build 
Application Objects.</p>
<p>Similar abstraction is used by the various Acceptance Testing tools,
such as FitNesse, Cucumber, and the other Gherkin tools - the spec texts
contain terms and values important for the business domain, and there
is separate code translating the spec's values and terms to call into 
the application and transforming its state into a format
expected by the tool.</p>
<h1 id="stripped-down-tests-only-the-script">Stripped down tests - only the script</h1>
<p>As seen above, the AT tools separate application logic from the test 
scenario's description.</p>
<p>Assertions have also been separated from test cases - either by developer 
choice, choosing to use a separate Assertions library like Hamcrest, 
instead of the unit testing library's own <code>assertFoo</code> methods), or
explicitly (<a href="http://visionmedia.github.io/mocha/#assertions">Mocha ships without an assertions library</a>).</p>
<p>Thus tests can really be focused just on the scenario being tested.</p>
<h1 id="fast-tests">Fast tests</h1>
<p>The single biggest disadvantage of <a href="http://www.confreaks.com/videos/641-gogaruco2011-fast-rails-tests">end to end tests is their speed</a>. They are slow. And the more of them there are, the slower they are.</p>
<p>This is one reason why the <a href="http://martinfowler.com/bliki/TestPyramid.html">Test Pyramid</a> recommends not having 
too many of them. Many architectural approaches (<a href="http://alistair.cockburn.us/Hexagonal+architecture">hexagonal</a>, DDD, etc.) 
suggest keeping a lightweight core application, and to attach the 
persistence and UI layers to it at its boundaries, leaving these ports 
and adapters lightweight too. Most of the testing then happens against
the core, dependency independent code, making the tests fast. </p>
<h1 id="fast-end-to-end-tests">Fast end to end tests</h1>
<p>Drumroll... we'll do a bit of cheating, of course. </p>
<p>Not all the tests have to run every single time. Performance tests are
usually not done when TDDing - that kicks in either later in 
the deployment pipeline, or runs daily. Teams organize their tests into
fast, smoke, and slow suites. Locally (and as the first step in the build
process) only the fast and smoke tests are run.</p>
<p>Putting all the above together means that writing systems with two 
self- contained cores (the app domain itself and the test scenarios) 
easily lends itself to end-to-end testing that can be run on multiple 
configurations, to give confidence that the app works with all its 
components and dependencies in production - yet enable fast feedback
required for developers. The same tests can be run:</p>
<ul>
<li>directly against the core application with mocks, stubs, etc.</li>
<li>through the app's (http) UI via the given frameworks/libraries testing 
  tools (e.g.: <code>django.test.client.Client</code>) with an in-memory database</li>
<li>through selenium against the full stack</li>
</ul>
<p>And of course, we can mix and match - selenium against SQLite, etc.</p>
<p>While TDDing, one can run the tests only against the fast core, after
that is complete, we can run the relevant tests with the end-to-end
driver, fix any mistakes that occur, check in, and let the build server
run all the integrated tests (using existing build practices to 
achieve speed)!</p>
<h2 id="in-which-contexts-could-it-make-sense">In Which Contexts Could It Make Sense?</h2>
<p>Thank you for reading this far - assuming you didn't just scroll 
ahead :)</p>
<p>The below list is by no means exhaustive, and as mentioned in the
introduction, there might be alternative approaches (please, let me
know!) - it's not a coincidence this blog is called "Exercises in 
public learning"! </p>
<p>With that out of the way, here are some contexts where this approach
could make sense:</p>
<ul>
<li>
<p>Working with a team where the skills both for testing and for writing
    good code are (yet) missing (<a href="http://5whys.com/blog/the-3-maturity-stages-of-a-software-team-and-how-scrum-fails.html">Chaotic team phase</a>).</p>
<p>As the joke goes, the only way to eat the elephant is one bite at a 
time. Same goes for learning - people can be overwhelmed to make 
the mental jump from manual to automated testing - throwing in good
programming practices can be too much.</p>
<p>Getting started with end to end tests that have decoupled driver
methods (even if on the <code>TestCase</code> class itself) is a great start - by
the time the tests become slow, if the team is bought into the idea
of automated testing, it can be refactored towards a core domain -
and inside that domain there still doesn't have to be proper clean
code (one step at a time).</p>
<p>In brief: for slow, gradual improvement.</p>
</li>
<li>
<p>The app actually has multiple interfaces for the same thing.</p>
<p>It can be due to A/B testing, or simply to accommodate the different
needs of different users (e.g.: for a webshop - there is the public
shop, the internal UI geared at the company's sales people, and the
API), multi-platform application (e.g.: mobile and desktop web, iOS and 
Android), etc.</p>
<p>If you test the checkout process end-to-end, then running the same
set of tests against each UI makes sense too - a single set of tests
to maintain and you know immediately whether all features work 
across all the views.</p>
</li>
<li>
<p>Catching unexpected bugs.</p>
<p>There is a class of bugs that can be caught by rigor, but I do slip
occasionally, ending up in a place where the unit tests are all 
green, but the application itself doesn't actually work.</p>
<p>Some real life such bugs I have run into:</p>
<ul>
<li>forgetting to place the actual input element on the page</li>
<li>encoding-persistence issues - an utf-8 database with a column
    that is windows-1250 encoded is ... unexpected</li>
<li>synchronizing data with another database where after the 
    required mappings it turned out said other database truncates
    our data</li>
</ul>
<p>All of the above can be addressed retroactively via adding targeted
tests for that specific integration point, but if we are already
testing the corner cases (length, encoding, etc.) in our code, it is 
nicer not to have to learn about these "unknown unknowns" from 
production problems...</p>
</li>
<li>
<p>Finally some related posts from other people:</p>
<ul>
<li>Ayende has multiple posts: on <a href="http://ayende.com/blog/154273/limit-your-abstractions-and-how-do-you-handle-testing">swapping out the infrastructure</a>, <a href="http://ayende.com/blog/4218/scenario-driven-tests">separating assertions from tests</a>, and <a href="http://ayende.com/blog/4217/even-tests-has-got-to-justify-themselves">about which tests add value in his opinion</a></li>
<li><a href="http://codebetter.com/sebastienlambla/2013/07/11/unit-testing-is-out-vertical-slice-testing-is-in/">Sebastien Lambla on Vertical Testing</a></li>
</ul>
</li>
</ul>
<p>There is much more to be said about other benefits of end to end 
testing, but this post is already too long, so that will have to wait
for another post (while waiting, you can read <a href="http://codemanship.co.uk/parlezuml/blog/?postid=1183">Jason Gorman's 101 Uses for Polymorphic testing</a>)!</p>
<p>P.S.: I would like to thank (in (first name based) alphabetical order): 
<a href="https://twitter.com/ajmolenaar">Arjan Molenaar</a>,
<a href="https://twitter.com/sietstweets">Cirilo Wortel</a>,
<a href="http://douglassquirrel.com/">Douglas Squirrel</a>,
<a href="http://twitter.com/jtf">Jeffrey Frederick</a>,
<a href="https://twitter.com/KishenPanday">Kishen Simbhoedatpanday</a>,
<a href="http://twitter.com/marcoemrich">Marco Emrich</a>,
<a href="https://twitter.com/mfeathers">Michael Feathers</a>,
and of course my colleagues at <a href="http://www.paessler.com">Paessler AG</a> -  I much appreciate that you 
all listened to me while I tried to figure out how to explain this and 
gave feedback both about the content and the format (*). Thank you!</p>
<hr />
<p>(*) just to be crystal clear, this does not mean they endorsed it, 
just that they listened and gave feedback!</p>]]></content:encoded>
    </item>
    <item>
      <title>Teach me refactoring from my commits!</title>
      <link>http://blog.zsoldosp.eu/2013/09/27/teach-me-refactoring-from-my-commits/</link>
      <pubDate>Fri, 27 Sep 2013 09:40:00 CEST</pubDate>
      <category><![CDATA[things i wish existed]]></category>
      <category><![CDATA[teaching]]></category>
      <category><![CDATA[untested ideas]]></category>
      <category><![CDATA[version control]]></category>
      <category><![CDATA[refactoring]]></category>
      <guid isPermaLink="false">GZ4JOnYZvQe5D8VUfKUh6eJAlkw=</guid>
      <description>Teach me refactoring from my commits!</description>
      <content:encoded><![CDATA[<p>I find it hard to learn purely by abstract theory - I need practical 
examples to illustrate the theory I just learned. I also need practice,
and preferably, supervised practice, so my mistakes can be caught early
and/or more efficient ways can be shown to achieve the same thing I just
did.</p>
<p>With this background, it's not surprising that when I <a href="http://episodes.gitminutes.com/2013/06/gitminutes-14-pablo-santos-on.html">heard about a 
product that started to do semantic diffs for version 
control</a> I was reminded of the <a href="https://sc2010subs.wordpress.com/2010/08/13/refactoring-golf-dave-cleal-ivan-moore/">Refactoring 
Golf</a> concept from the first <a href="http://www.codemanship.co.uk/parlezuml/softwarecraftsmanship/">Software Craftsmanship 
London</a> conference - and the two ideas just clicked into this
great (ok, I'm biased) idea to try to derive the most efficient
refactoring steps for that particular commit I just made.</p>
<p>The pieces required for this project are all already in place:</p>
<ul>
<li>we have parsers for languages, and many are available as libraries to
  traverse <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">ASTs</a></li>
<li>most version control systems have libraries to read from them</li>
<li>there are tons of well-defined refactorings known, that can be used
  as operations in the transformations from state <code>A</code> to <code>B</code></li>
<li>optimization algorithms are plentiful - for a proof of concept, the
  <a href="https://en.wikipedia.org/wiki/String_distance">String Distance</a> Transformation algorithm could be used</li>
</ul>
<p>Thus such a program script could be added as a post commit hook, or 
simply could be run on the workspace copy, using the diff against last
committed version, and it could tell me that I might have:</p>
<ol>
<li>renamed variable <code>login</code> to <code>username</code></li>
<li>converted <code>username</code> from local variable to method parameter</li>
<li>extracted method <code>lock_customer_acocunt</code> from method <code>login</code></li>
</ol>
<p>I'm pretty positive even experienced refactoring practitioners could 
learn new tricks, and newbies would be delivered concrete refactoring
examples tailored to their very codebase.</p>
<p>Please, someone with enough free time, go and make such an app!</p>
<p>Does this idea make sense for you too, or just me? Let me know via email
(hello at site domain) or on twitter (<a href="https://twitter.com/zsepi">@zsepi</a>)!</p>]]></content:encoded>
    </item>
    <item>
      <title>Don't repeat yourself, even across platforms</title>
      <link>http://blog.zsoldosp.eu/2010/09/dont-repeat-yourself-even-across.html</link>
      <pubDate>Sun, 26 Sep 2010 12:57:00 CEST</pubDate>
      <category><![CDATA[untested ideas]]></category>
      <category><![CDATA[programming]]></category>
      <category><![CDATA[software]]></category>
      <guid isPermaLink="false">http://blog.zsoldosp.eu/2010/09/dont-repeat-yourself-even-across.html</guid>
      <description>Don't repeat yourself, even across platforms</description>
      <content:encoded><![CDATA[
<div>It's well known that duplication (not even mentioning triplication, quadruplication , etc.) is bad for maintainability (hence the <a href="http://c2.com/cgi/wiki?DontRepeatYourself">DRY principle</a>).  I have not seen much talk around DRY with regards to multiple platforms/programming languages, so this post is my attempt to distill my thoughts and to learn about the pros/cons of applying this principle across languages/platforms. </div><h2>Why would you use multiple platforms, or do a polyglot project?</h2><div>Without the goal of enumerating all possible reasons, some case are:<br/><ul><li>Web applications need the same input sanity validation performed both client side for usability (JavaScript) and server side (whatever technology you use) for security. The same argument can be made for any N-tier application for DTOs.</li><li>For a portfolio of applications in the same domain, there is a need for consistency  - e.g.: reference data catalog for input fields (you want to use the same terminology across the applications)</li><li>There can be similar logic applicable to different platforms - e.g.: some static code analysis is the same across platforms, whether or not we talk about Java or C# code, and it'd be nice to have just a single implementation.</li></ul></div><div><div><h2>Some approaches</h2><div><b>Meta data driven</b> libraries- the canonical data source is stored in some kind of a database, which contains a cross-platform implementation of the logic (e.g.: validation logic saved as regular expressions) and you have a minimal implementation in each language that is generic and data driven. There is minimal amount of code that your need to depend on in your application, which makes it a stable dependency (note I'm not talking about the code stored in the database, just the API you program against). However, the actual validation logic becomes black box from the testing perspective, and any of these metadata severely limits what you can do with it, and extension points are much harder to find - e.g.: should you want to restrict your application to only accept a certain range of zip code, how do you build that into a regexp based data driven validation framework? It certainly is possible, but hard, and in many cases expanding the framework  increases complexity with significantly, and the data dictionary becomes somewhat hard to maintain.</div><div><br/></div><div><b>Plain Old X Objects</b>* appeal more to me. They are easily debuggable, can be used in isolation and offline development. They can be much richer (you can have all the errors encountered reported back to the user rather than a pass/fail), more readable (though certainly you can <a href="http://www.codinghorror.com/blog/2008/06/regular-expressions-now-you-have-two-problems.html">write readable regular expressions</a>), and should you want to enhance/override the default behavior in special cases, you can easily override them. The problem of course is to make the same logic available on multiple platforms, in a unified fashion.</div><div><br/></div><div>If the technology stack allows it,  <b>pick a language that runs on all parts of the stack</b> (e.g.: Ruby/Python/JavaScript are all available as standalone and for the CLR/JVM.) The integration tests are fairly easy, just run the same tests with the different platforms.</div><div><br/></div><div>If there is no suitable bridge for the stack, <b>code generation</b> is one option, which is almost the same concept as the metadata driven simple framework above, just taking out the datastore and generating classes for each of the rules to enable offline usage, debugging, etc. This has the additional cost of creating and evolving the code generator tool in addition.</div><h2>Testing</h2><div>Irregardless of the path chosen, the shared logic must be tested and documented on all platforms, which might be hard to do. Acceptance testing tools (fitnesse, concordion, etc.) can help, or for the data driven tests (e.g.: for validation, input string, should be valid, expected error message) a simple testrunner can be created for each of the platforms. </div><div><div><h2>(Potential) Problems</h2><div><ul><li>Diversity vs. monoculture. The library becomes a single point of failure, and any bug has far reaching consequences. On the other hand, the reverse is true: any problem has to be fixed only once, and the benefits can be reaped by all that use the library. However, there might be fewer people looking at the shared domain for corner cases...</li><li>Shared dependency overhead - shared libraries can slow down development both for the clients of the library and the library itself. Processes for integration must be in place, etc. Gojko Adzic has a great post on <a href="http://gojko.net/2009/09/02/to-merge-or-not-to-merge-code-essentially-not-an-it-decision/">shared library usage</a>.</li><li>False sense of security - users of the library might assume that's all they need to do and not think through every problem so carefully. E.g.: DTO validation library might be confused with entity/business rules validation</li><li>Ayende has recently written a post about <a href="http://ayende.com/Blog/archive/2010/09/09/maintainability-code-size-amp-code-complexity.aspx">Maintainability, Code Size &amp; Code Complexity</a> that is (slightly) relevant to this discussion (<i><span class="Apple-style-span" style="font-size: small;">"The problem with the smaller and more complex code base is that the complexity tends to explode very quickly."</span></i>). In my reading the points are more applicable for the data-driven (or from there code generated) approach, where that smart framework becomes overly complex and fragile. Note he talks about a single application, and it's known that when dealing with a portfolio (NHProf, EFProf, etc.), he chose to  use a single base infrastructure.</li></ul></div><div><br/></div><div>Have you done something similar in practice? What are your thoughts and experiences? Or have you been thinking about this very same topic? What have I missed? What have I misunderstood? Let me know!</div><div><br/></div></div></div></div><div><br/></div><div>* <a href="http://www.martinfowler.com/bliki/POJO.html">http://www.martinfowler.com/bliki/POJO.html</a></div></div>]]></content:encoded>
    </item>
  </channel>
</rss>
